# Fine-Tuning Lightweight LLMs for a Bilingual DSP Teaching Assistant

## Introduction to Data Science - SEEU (2025)

**Author**: Irfan Uruchi
**Professor**: Asst. Prof. Dr. Nuhi Besimi

---

# Project overview

This repository contains the complete code, dataset samples and training pipeline used to fine-tune a lightweight LLaMA 3.2–1B model into a bilingual (English–Albanian) Digital Signal Processing (DSP) teaching assistant.

The project demonstrates that:

- Small models (1B parameters) can perform well on DSP tasks.
- High-quality synthetic verified data is more important than raw model size.
- QLoRA allows full fine-tuning within 5–6GB VRAM.
- A bilingual assistant is possible even in low-resource languages like Albanian.

This repository supports full reproducibility.


